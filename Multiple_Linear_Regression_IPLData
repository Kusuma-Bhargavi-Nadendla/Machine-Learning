

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn import metrics
import matplotlib.pyplot as plt

data=pd.read_csv("IPL_Dataset.csv")
data.info()

data.iloc[0:5,15:22] #get 5 rows with 12 columns

data.columns

#extract features
x_features=[ 'AGE', 'COUNTRY','PLAYING ROLE',
       'T-RUNS', 'T-WKTS', 'ODI-RUNS-S', 'ODI-SR-B', 'ODI-WKTS', 'ODI-SR-BL',
       'CAPTAINCY EXP', 'RUNS-S', 'HS', 'AVE', 'SR-B', 'SIXERS', 'RUNS-C',
       'WKTS', 'AVE-BL', 'ECON', 'SR-BL']
y_features=['SOLD PRICE']

#encode categorical variables
categ_v=['AGE', 'COUNTRY','PLAYING ROLE','CAPTAINCY EXP']
encoded_data=pd.get_dummies(data[x_features],columns=categ_v,drop_first=True)
encoded_data.columns

encoded_data

encoded_data.iloc[0:5,15:]

X=sm.add_constant(encoded_data)
Y=data[y_features]

train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=50)

ipl_model=sm.OLS(train_y,train_x).fit()

ipl_model.summary()

pred_y=ipl_model.predict(test_x)
r2value=metrics.r2_score(pred_y,test_y)
np.abs(r2value)

#rmse value
rmse=np.sqrt(metrics.mean_squared_error(pred_y,test_y))
rmse

plt.scatter(pred_y,test_y)

#Accuracy is quite low. So we need to handle Multicollinearity by variance inflation factor
from statsmodels.stats.outliers_influence import variance_inflation_factor
def get_vif_values(x):
  vif=[variance_inflation_factor(x.values,i) for i in range(x.shape[1])]
  vif_factors=pd.DataFrame()
  vif_factors['column']=x.columns
  vif_factors['vif']=vif
  return vif_factors

vif_fac=get_vif_values(encoded_data)
vif_fac

col_large_vif=vif_fac[vif_fac['vif']>4].column
col_large_vif

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
plt.figure(figsize=(12,10))
hm=sns.heatmap(X[col_large_vif].corr(),annot=True)

col_to_be_removed=['T-RUNS','T-WKTS', 'RUNS-S','HS',  'AVE',  'RUNS-C', 'SR-B', 'AVE-BL', 'ECON',  'ODI-SR-B', 'ODI-RUNS-S', 'AGE_2','SR-BL']
new_x_features=list(set(encoded_data.columns)-set(col_to_be_removed))
new_x_features

new_vif=get_vif_values(X[new_x_features])
new_vif

#We can see that all the new variables have VIF values less than 4 only which indicates no multicollinearity.

#build new model with new features
train_x=train_x[new_x_features]
test_x=test_x[new_x_features]
model2=sm.OLS(train_y,train_x).fit()

model2.summary()

#make predictions
pred_y=model2.predict(test_x)
r2_value2=metrics.r2_score(test_y,pred_y)
print('New R2 Value is ',np.abs(r2_value2))
#We can see that the performance of the model increased after removal of Multi collinearity.

model2.summary()

#From the above p-values only 4 features :COUNTRY_IND, COUNTRY_ENG, SIXERS, CAPTAINCYEXP_1 have p-value<0.05 and are significant
#So  let's build a new model with these significant features
significant_features=['COUNTRY_IND', 'COUNTRY_ENG', 'SIXERS', 'CAPTAINCY EXP_1']
train_x=train_x[significant_features]
test_x=test_x[significant_features]
model3=sm.OLS(train_y,train_x).fit()
model3.summary()

pred_y=model3.predict(test_x)
r2_value3=metrics.r2_score(test_y,pred_y)
print('R2 value for Model3:',np.abs(r2_value3))

# Commented out IPython magic to ensure Python compatibility.

#Probability plot (Test for normality of residuals)
import matplotlib.pyplot as plt
from scipy.stats import probplot
# %matplotlib inline
def draw_pp_plot( model, title ):
    probplot( model.resid,plot=plt);
draw_pp_plot(model3,"");

#Residual Plot for Homoscedasticity

def get_standardized_values(vals):
  return (vals-vals.mean())/vals.std()
def plot_resid_fitted(fitted, resid, title):
    plt.scatter( get_standardized_values( fitted ),
    get_standardized_values( resid ) )

    plt.title(title)
    plt.xlabel("Standardized predicted values")
    plt.ylabel("Standardized residual values")
    plt.show()
plot_resid_fitted(model3.fittedvalues,model3.resid," Residual Plot")

#Since there is no shape of funnel, Heteroscedasticty is absent

#Detecting influencers

k = train_x.shape[1]
n = train_x.shape[0]
print("Number of variables:", k, " and number of observations:", n)
leverage_cutoff = 3*((k + 1)/n)
print( "Cutoff for leverage value:", round(leverage_cutoff, 3) )

from statsmodels.graphics.regressionplots import influence_plot
fig, ax = plt.subplots( figsize=(8,6) )
influence_plot(model3, ax = ax )
plt.title( " Leverage Value Vs Residuals" )
plt.show()
'''From the diagram, shows there are three observations 23, 58, 83 that
 have comparatively high leverage with residuals. We can filter out the influential observations'''

data[data.index.isin( [23, 58, 83] )]

train_X_new = train_x.drop([23, 58, 83], axis = 0)
train_y_new = train_y.drop([23, 58, 83], axis = 0)




train_y = np.sqrt(train_y)
ipl_model_4 = sm.OLS(train_y, train_x).fit()
ipl_model_4.summary2()




draw_pp_plot(ipl_model_4,"Figure 4.9 - Normal P-P Plot of Regression Standardized Residuals");




pred_y = np.power(ipl_model_4.predict(test_x[train_x.columns]),2)
pred_y




from sklearn import metrics
np.sqrt(metrics.mean_squared_error(pred_y, test_y))




np.round( metrics.r2_score(pred_y, test_y), 2 )
