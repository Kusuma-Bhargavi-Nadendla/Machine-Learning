# -*- coding: utf-8 -*-
"""ML LAB1 SIMPLE LINEAR REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Ag3FNK3-CNqBtrcH70Z_QopaBqBAG80
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
# %matplotlib inline
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error, accuracy_score, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

#reading data
data=pd.read_csv('MBA Salary.csv')
print(data)

#print shape and head
print(data.shape)
print(data.head())
print(data.tail())
print(data.info())
print(data.describe())

#Get input and output columns
X=data['Percentge in Grade 10']
Y=data['Salary']
plt.scatter(X,Y)

# need to add the constant b_0  to the equation using the add_constant() method.
X=sm.add_constant(X)
print(X,Y)

#split the datset
train_X,test_X,train_y,test_y=train_test_split(X,Y,train_size=0.8,random_state=100)

#fit the ols(Ordinary least squares) model
salary_model=sm.OLS(train_y,train_X).fit()

#print the estimated parameters
print(salary_model.params)

print("For every 1% increase in Grade 10, the salary of the MBA students will increase by ",salary_model.params[1])

#print the summary
print(salary_model.summary())
#greater the r2 value , better the model is.

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sn
from scipy.stats import probplot
# %matplotlib inline
mba_salary_resid = salary_model.resid
probplot(mba_salary_resid,plot=plt)
#probplot = sm.ProbPlot(mba_salary_resid)
plt.figure( figsize = (8, 6))
#probplot.ppplot( line='45' )

#Test for Homoscedasticity
def standardize(vals):
  return (vals-vals.mean())/vals.std()
plt.scatter(standardize(salary_model.fittedvalues),standardize(salary_model.resid))
plt.title("Test for Homoscedasticity")
plt.show()
#If funnel shape exists, Heteroscedasticity

#Outlier analysis
#Z-score analysis
from scipy.stats import zscore
data['z_score']=zscore(data['Salary'])
data[(data['z_score']>3.0) | (data['z_score']<-3.0)]
#No outliers

#Cook's distance
influence=salary_model.get_influence()
(c,p)=influence.cooks_distance
plt.stem(np.arange(len(train_X)),np.round(c,3),markerfmt=",")
plt.title("Cooks distance")
plt.show()

from statsmodels.graphics.regressionplots import influence_plot
import matplotlib.pyplot as plt
fig,ax=plt.subplots(figsize=(8,6))
influence_plot(salary_model,ax=ax)
plt.title("Figure -Leverage value vs Residuals")
plt.show()

#predict with percentage
salary_model.predict(np.array([50,1]))

#compare actual values of test dataset and predictions
performance=pd.DataFrame()
performance['Predictions']=salary_model.predict(test_X)
performance['Actual results']=test_y
performance

# Commented out IPython magic to ensure Python compatibility.
#chcek for normal distribution
import matplotlib.pyplot as plt
import seaborn as sn
# %matplotlib inline

salary_model.resid

#plot with predicted vs actual values
plt.scatter(performance['Predictions'],performance['Actual results'])

#find r2 score of model
pred_y=salary_model.predict(test_X)
np.abs(r2_score(test_y,pred_y))

#rmse
import sklearn
np.sqrt(sklearn.metrics.mean_squared_error(test_y,pred_y))

#Now let's remove outliers to improve the performance of modal.
def remove_outliers_iqr(data):
  q1=data.quantile(0.25)
  q3=data.quantile(0.75)
  iqr=q3-q1
  lower_bound=q1-1.5*iqr
  upper_bound=q3+1.5*iqr
  return (data>=lower_bound) & (data<=upper_bound)

data_model2=data[remove_outliers_iqr(data['Salary'])]
data_model2

#now build the model again
new_x=data_model2['Percentge in Grade 10']
X=sm.add_constant(new_x)
Y=data_model2['Salary']
train_x,test_x,train_y,testy=train_test_split(X,Y,train_size=0.8,random_state=200)
model2=sm.OLS(train_y,train_x).fit()
y_pred=model2.predict(test_x)
print('New R2 Score is ',np.abs(r2_score(test_y,y_pred)))
#New model works with improved accuarcy.









#Using linear regression
from sklearn.linear_model import LinearRegression
lm=LinearRegression()
lm.fit(train_X,train_y)

#plot with predicted vs actual values
predictions=lm.predict(test_X)
plt.scatter(test_y,predictions)

#r2 value
r2_value=lm.score(test_X,test_y)
print('R2 Score of model is ',r2_value)

"""Model Diagnostics"""

#find r2 score of model
pred_y=salary_model.predict(test_X)
r2_score(test_y,pred_y)

#Root mean square error is the average error the model makes. Lesser the value, better the model.
rmse=np.sqrt(mean_squared_error(test_y,pred_y))